\newpage
\begin{center}
  \textbf{\large ПРИЛОЖЕНИЕ А}
\end{center}
\refstepcounter{chapter}
\addcontentsline{toc}{chapter}{ПРИЛОЖЕНИЕ А}

Очевидно неудачный эксперимент с neftune CV онли.

Nemo = 1.6994516301254496, Whisper = 1.6890259441571887

\begin{lstlisting}[language=Python,label={lst:train},caption=train.py,breaklines=true,]
import dotenv
import hydra
import wandb
from datasets import load_dataset
from peft import get_peft_model
from transformers import (
  Seq2SeqTrainer,
  Seq2SeqTrainingArguments,
  DataCollatorForSeq2Seq
)
from utils import (
  compute_wer,
  create_batch_prompt,
  get_lora_config,
  load_model_tokenizer,
  preprocess_logits_for_metrics
)
import os
from functools import partial

dotenv.load_dotenv()

@hydra.main(config_path="configs", config_name="train", version_base="1.3")
def main(cfg: dict) -> None:
  llm, tokenizer = load_model_tokenizer(cfg.model)
  create_prompt_p = partial(create_batch_prompt, tokenizer=tokenizer)
  compute_wer_p = partial(compute_wer, tokenizer=tokenizer)

  # take data and preprocess
  train_dataset = load_dataset(
      'json',
      data_files=cfg.data.train_dataset
    ).shuffle(seed=42)
    val_dataset = load_dataset('json', data_files=cfg.data.val_dataset)
    train_dataset = train_dataset.map(
        create_prompt_p,
        batched=True,
        batch_size=64,
        remove_columns=["n_best", "gt"],
        num_proc=16
      )
    val_dataset = val_dataset.map(
        create_prompt_p,
        batched=True,
        batch_size=64,
        remove_columns=["n_best", "gt"],
        num_proc=16
      )
    
    # logging
    wandb.login(
      key=os.getenv("WANDB"), 
      anonymous="allow",
    )
    wandb.init(**cfg.wandb_proj)

    # training
    llm = get_peft_model(llm, get_lora_config(cfg.lora))
    training_arguments = Seq2SeqTrainingArguments(**cfg.trainer)
    trainer = Seq2SeqTrainer(
      model=llm,
      tokenizer=tokenizer,
      data_collator=DataCollatorForSeq2Seq(
              tokenizer=tokenizer,
              model=llm
          ),
      args=training_arguments,
      train_dataset=train_dataset['train'],
      eval_dataset=val_dataset['train'],
      compute_metrics=compute_wer_p,
      preprocess_logits_for_metrics=preprocess_logits_for_metrics,
    )
    trainer.train()
    trainer.model.save_pretrained('./models/'+cfg.name)
    wandb.finish()

if __name__ == "__main__":
  main()
\end{lstlisting}

\begin{lstlisting}[language=Python,label={lst:eval},caption=eval.py,breaklines=true,]
import evaluate
import hydra
import torch
import pandas as pd
from tqdm import tqdm
from peft import PeftModel
from transformers import GenerationConfig, AutoModelForSeq2SeqLM
from utils import load_model_tokenizer, create_prompt

import json


@hydra.main(config_path="configs", config_name="val", version_base="1.3")
def main(cfg: dict) -> None:
  generation_config = GenerationConfig(**cfg.generate)
  device = "cuda" if torch.cuda.is_available() else 'cpu'

  llm, tokenizer = load_model_tokenizer(cfg.model)
  if "fred" in cfg.model.name:
    llm = AutoModelForSeq2SeqLM.from_pretrained(cfg.model.name)
  else:
    llm = PeftModel.from_pretrained(llm, cfg.ckpt)
  llm = llm.to(device)
  
  llm.eval()
  if device == "cuda":
    llm = torch.compile(llm)
  wer = evaluate.load("wer")

  with open(cfg.data.test_dataset, "r", encoding="utf-8") as file:
    data = json.load(file)

  output = {
    "gt": ['' for _ in range(len(data))],
    "asr": ['' for _ in range(len(data))],
    "asr_wer": [0. for _ in range(len(data))],
    "llm": ['' for _ in range(len(data))],
    "llm_wer": [0. for _ in range(len(data))],
  }
  for i, sentences in enumerate(tqdm(data)):
    if "fred" in cfg.model.name:
      input_ids = tokenizer.encode(
          sentences['n_best'][0],
          add_special_tokens=True,
          return_tensors='pt'
        )
    else:
      input_ids = create_prompt(sentences, tokenizer)
    
    gt = sentences['gt']

    with torch.inference_mode():
      if "fred" in cfg.model.name:
        generation_output = llm.generate(
            **input_ids.to(llm.device),
            max_length=input_ids["input_ids"].size(1) * 1.5
          )
      else:
        generation_output = llm.generate(
          input_ids=input_ids.to(device),
          generation_config=generation_config,
          return_dict_in_generate=False,
          output_scores=False,
        )

    llm_output = tokenizer.decode(generation_output.to('cpu')[0], skip_special=True)
    llm_output = llm_output.replace("<pad>", "").replace("</s>", "").strip()
    
    output['gt'][i] = gt
    output['asr'][i] = sentences['n_best'][0]
    output['asr_wer'][i] = wer.compute(
        predictions=[output['asr'][i]],
        references=[output['gt'][i]]
      )
    output['llm'][i] = llm_output
    output['llm_wer'][i] = wer.compute(
        predictions=[output['llm'][i]],
        references=[output['gt'][i]]
      )
  
  pd.DataFrame(output).to_csv("output_nemo_cv.csv", index=False)

if __name__ == "__main__":
  main()
\end{lstlisting}

\begin{lstlisting}[language=Python,label={lst:generate},caption=generate\_hypotheses.py,breaklines=true,]
from omegaconf import OmegaConf
import hydra
import nemo.collections.asr as nemo_asr
import pandas as pd
from tqdm import tqdm

import copy
import json
import random
from pathlib import Path

from utils import NoStdStreams, silent_logs

@hydra.main(config_path="configs", config_name="generate_hp", version_base="1.3")
def main(cfg: dict) -> None:
  silent_logs()
  asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(
      model_name="nvidia/stt_ru_fastconformer_hybrid_large_pc"
    )
  config = OmegaConf.load("configs/nemo/fast-conformer_transducer_bpe.yaml")
  decoding_config = copy.deepcopy(config.model.decoding)
  decoding_config.strategy = cfg.nemo.strategy
  decoding_config.beam.beam_size = cfg.nemo.beam_size
  decoding_config.beam.return_best_hypothesis = cfg.nemo.return_best_hypothesis
  asr_model.change_decoding_strategy(decoding_config)

  for split in ["train"]:
    if "cv" in cfg.data.path:
      base_path = Path(cfg.data.path)
      df = pd.read_csv(Path(base_path, f"{split}.tsv"))
      base_path = Path(base_path, "clips")

      df = df[df['exist']]
      pathes = [str(Path(base_path, path)) for path in df['path'].to_list()]
      transcriptions = df['sentence'].to_list()
      del df
      iterable = zip(pathes, transcriptions)
    else:
      base_path = Path(cfg.data.path, split)
      with open(Path(base_path, "manifest.json"),'r',encoding="utf-8") as file:
          iterable = file.readlines()

    all_data = []

    for i, it in enumerate(tqdm(iterable)):
      if "cv" in cfg.data.path:
        path, gt = it
      else:
        it = eval(it)
        path = str(base_path)+"/"+it['audio_filepath']
        gt = it["text_no_preprocessing"]
      with NoStdStreams():
        all_hypotheses = asr_model.transcribe(path, batch_size=1)
      n_best = []
      for hypothesis in all_hypotheses[0]:
        if hypothesis.text not in n_best:
          n_best.append(hypothesis.text.strip())
          if len(n_best) == 5:
            break

      # sampling
      while len(n_best) < 5:
        n_best.append(random.choice(n_best))

      gt = gt.strip().replace('""', '"')
      if gt.startswith('"') and gt.endswith('"'):
        gt = gt[1:-1]

      all_data.append({
        "n_best": n_best,
        "gt": gt
      })
      if i % 200 == 0:
        with open(Path(base_path, f"{split}.json"),"w",encoding="utf-8") as file:
          json.dump(all_data, file)

    with open(Path(base_path, f"{split}.json"),"w",encoding="utf-8") as file:
      json.dump(all_data, file)

if __name__=="__main__":
  main()
\end{lstlisting}
