\newpage
\begin{center}
  \textbf{\large 1. АНАЛИЗ ПРЕДМЕТНОЙ ОБЛАСТИ ДЛЯ РАЗРАБОТЧИКА ПО}
\end{center}
\refstepcounter{chapter}
\addcontentsline{toc}{chapter}{1. АНАЛИЗ ПРЕДМЕТНОЙ ОБЛАСТИ ДЛЯ РАЗРАБОТЧИКА ПО}


\section{Общие сведения об области распознавания речи}

Распознание речи - это развивающаяся область машинного обучения.
Оно представляет из себя преобразования аудио сигналов в текст, то есть транскрибации.
Данная задача играет важную роль в самых разных областях.
Оно задействовано в голосовых ассистентах и умных домах, транскрибации звонков в колл-центрах и в онлайн созвонах, может быть использована в определение эмоций и так далее.

Прежде чем перейти к описанию методов, необходимо уточнить, как происходит оценка качества работы моделей.
В отличие от языковых систем перевода из одного языка в другой, где порядок слов может и не играть важной роли, в системах аудио порядок слов и правильность их распознавания играют важную роль.
Основным методом для оценки качества работы алгоритма транскрибации используют расстояние Левинштейна.
Данная метрика подсчитывает своеобрзный замер расстояния между словами.
В общем виде на основе двух текстовых последовательностей: целевой и полученной алгоритмом машинного обучения - идёт расчёт четырёх числовых метрик.
Значения последовательность принимает $[0, +\inf]$, где 0 - последовательности идентичны.
Общая формула представлена в уравнение~\ref{eq:levinstein}:

\begin{equation}
  Levinstein Distance = \frac{S + D + I}{N} = \frac{S + D + I}{S + D + C}
  \label{eq:levinstein}
\end{equation}

где $S$ обозначает количество замен, $D$ - удалений, $I$ - вставок и $C$ - правильных слов. 
Если количество слов в исходной и полученной последовательностях равны, но слова разные, то они считаются как замена. 
Если слов в исходной последовательности больше, то разница считается как удаление.
Для случая, когда количество слов в изначальной последовательности больше, чем в сгенерированной, разница будет считаться как вставка.
И случаи, когда слова в обоих последовательностях совпадают, считаются как правильная транскрибация.

Как можно понять из формулы, она даёт более плохой результат в случае, когда наша модель генерирует последовательность длины сильно больше оригинальной.
Также видно, что основная разница между последовательностями замеряется с помощью $I$ и $C$, в то время как две другие метрики скорее сглаживают результат.

Расстояние Левинштейна лежит в основе двух метрик, выбор между которыми сводится к выбору группы, к которой относится целевой язык.
\begin{itemize}
  \item Для китайского, японского и корейского языков используется Character Error Rate (CER), который считает расстояние между словами посимвольно.
  CER используется для групп языков, где одни и те же единицы языка в разном порядке могут давать совершенно разный результат.
  Также в этой группе отдельные слова и явления могут отображаться всего одним символом, поэтому тут важна точность каждой единицы языка по отдельности.
  \item Для русского, английского и немецкого используется Word Error Rate (WER), который уже считает расстояние между отдельными словами.
  Поскольку в этой группе языков буквы не играют по отдельности большой роли в формирование предложений, здесь в качестве основы для оценки качества принято использовать буквы.
\end{itemize}

В дальнейшем оценка качества моделей будет проводиться на основе WER, так как мы будем оценитвать качество для русского и английского.

\section{Данные для обучения}

Ещё одной немаловажной частью для области машинного обучения являются данные, на которых это всё обучается и оценивается.
Существует огромное количество самых различных датасетов, которые содержат аудио записи, относящиеся  к разным тематикам.
Поговорим сначала об английских.

\texttt{LibriSpeech} какое-то время был самым большим и разнообразным набором данных.
Он содержит в себе озвученные профессиональными спикерами адуиокниги.
Существуют вариации разбиения на 100, 360 и 500 часов.
Также датасет разбивается на clean и other.
Первый проще для распознания речи, второй сложнее.

\texttt{Common Voice} на данный момент крупнейший датасет.
Данные в него набираются волонтёрами по всему миру.
Волонтёры могут внести свой вклад несколькими способами.
Первый вариант - запись аудио.
Второй вариант - делать текстовые <<пожертвования>>, а именно записи фраз из книг, газет, добавление фразеологизмов и так далее.
Также на добровольных началах люди могут проверить правильность записанных аудио и текстовых данных.
Особенность в том, что Mozilla оплачивает хостинг всех этих данных, однако их проверкой может заняться каждый человек после регистрации на сайте.

Эти датасеты являются наиболее разнообразными по темам и набору спикеров, поэтому оценка качества распознавания для английского языка в основном проводится для них.

Для русского языка существуют свои русские версии вышеупомянутых \texttt{LibriSpeech} и \texttt{Common Voice}.
Также существуют несколько датасетов, собранных чисто для русского языка. 
Например, \texttt{DUSHA}.

\section{Методы машинного обучения для задачи распознания речи}

\textbf{Тут что-то про МЛ. Хз, как оно было до нейронок. ROVER там, мб Kaldi.}

В 2014 выходит статья про архитектуру Deep Speech от Baidu. 
Это модель, построенная на основе Recurrent Neural Network (RNN).
Их особенностью является возможность передачи скрытого состояния (hidden state) между ячейками.
Это состояние представляет из себя своеобразный контекст подобно тому, как человек читает текст и связывает нынешнее слово с предыдущим.
Данные модели также можно "развернуть" и пустить в обратную сторону для закрепления полученных слов.
В Baidu как раз использовали двунаправленные (bidirectional) RNN.
Во второй версии Deep Speech авторы сравнили два других подхода, основанных на RNN - Long-Short Term Memort (LSTM) и Gated Recurrent Unit (GRU).
У LSTM особенностью является то, что она передаёт сразу два контекста: общий от всех предыдущих слов (long) и от последнего слова (short).
Благодаря этому LSTM-модели могут лучше усваивать контекст последовательности.
Однако вместе тем это ведёт к повышенным затратам вычислительных ресурсов.
Для решения этой проблемы были созданы GRU.
У них, как и у RNN, существует только один вид памяти, однако мехазим забывания (gate) позволяет вытеснять из общей памяти информацию о старом контексте и заменять её на новую.


\textbf{ТУТ КАРТИНКУ с RNN архитектурами вставь}

По итогу при правильной инициализации bias у gate GRU показала себя лучше, чем классическая RNN и сопоставимо с LSTM.
Механизм работы у GRU можно выразить уравнением~\ref{eq:gru}:

\begin{equation}
  \begin{aligned}
    z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) \\
    r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r) \\
    \tilde{h}_t &= f(W_h x_t + r_t \circ U_h h_{t-1} + b_h) \\
    h_t &= (1 - z_t) h_{t-1} + z_t \tilde{h}_t
  \end{aligned}
  \label{eq:gru}
\end{equation}

Были попытки улучшения качества с использованием архитектурных решений.
Так появились Transducer (RNN-T) и Listen, Attend, Spell (LAS).
Традиционно модели обучались с использованием Connectionist Temporal Classification (CTC) функции потерь.
Поскольку в аудио отдельные звуки могут занимать разные по продолжительности сегменты, встала необходимость в выравнивание этих кусочков.
Так протяжные звуки иногда необходимо объединять вместе, как-то обозначать пустоту, когда речи в аудио нет.
Проблемой CTC является то, что при декодирование отсутствует контекстная информация, так как предполагается, что токены распределны независимо.
Также качество модели сильно зависит от внешней языковой модели.

\textbf{ТУТ CTC КАРТИНКА}

RNN-T в свою очередь добавляет к аудио энкодеру ещё два компонента - предиктор и joiner-сеть.
Так аудио кодируется в скрытое пространство с помощью аудио кодировщика.
Этот вывод затем передаётся в состояние $y$.
Оно далее используется в предикторе, который представляется собой каузальную сеть, основанную зачастую на рекуррентных или траснформерных сетях.
Каузальная означает, что она генерирует следующий токен, опираясь только на предыдущую информацию.
Выходы кодировщика и предиктора далее объединяются в полносвязной сети joiner и снова передаются в состояние $y$, которое потом используется в предикторе для новой генерации.

\textbf{ТУТ RNN-T КАРТИНКА}

Следующим большим словом в мире ASR систем стала модель Whisper от OpenAI.
Данная модель построена на основе архитектуры трансформер.
Эта архитектура была придумана Google в 2017 году в статье "Attention is all you need".
Суть архитектуры сводится к QKV attention механизму~\ref{eq:qkv}:

\begin{equation}
  \begin{aligned}
    z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) \\
    r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r) \\
    \tilde{h}_t &= f(W_h x_t + r_t \circ U_h h_{t-1} + b_h) \\
    h_t &= (1 - z_t) h_{t-1} + z_t \tilde{h}_t
  \end{aligned}
  \label{eq:qkv}
\end{equation}

Пусть у нас будет на вход матрица, состоящая из векторных представлений токенов $X$.
Мы умножаем её на матрицы весов $W_Q$, $W_K$ и $W_V$.
Так мы получаем наши матрицы Queries $Q$, Keys $K$ и Values $V$.
По своей сути первая матрица отображает закодированное значение слов, вторая матрица отображает значение, которые эти слова могут передать, а третья - дополнительную величину, на которую слова изменятся.
С помощью механизма внимания, трансформеры выделяют локальные признаки в последовательности.
Если говорить про текстоый домен, то они выделяют зависимости между отдельными словами-токенами.

Модель Whisper разделёна на два блока: аудио энкодер и текстовый декодер.
По своей работе текстовый декодер похож на методы рескоринга, о которых будем далее.
Можно сказать, что он представляет собой языковую модель (LM), которая авторегрессионно предсказывает следующий текстовый токен.
Мы делаем бесконечный цикл, передаём скрытое состояние декодера и предыдущие предсказанные слова, и генерируем новые до тех пор, пока не получим токен конца текста.

Проблемой работы модели Whisper является то, что она не имеет моделей обученных на русский язык.
Существуют мультиязычные версии, однако они начинаются с модели Medium, в которой 700+ миллионов параметров.
Можно найти версии, которые были обучены членами ИИ-сообщества, однако зачастую такие модели обучаются на маленьком наборе данных из-за ограничений в ресурсах.

Проблему решила Nvidia с выпуском серии fast-conformer моделей. 
Они архитектурно построен на Conformer модели, где свёрточные нейронные сети (CNN) используются в комбинации с трансформерами.
Свёрточные сети, придуманные в 1990 и использовавшиеся изначально с задачах компьютерного зрения Яном Лекуном в LeNet, проходятся окном свёртки по тензорам, выделяя какие-то общие визуальные паттерны.
С помощью этой методики мы получаем карты активаций, на которых показано распределение визуальных признаков на нашем изначальном тензоре.
Грубо говоря, мы выделаем глобальные признаки из изначального тензора.
А трансформеры, как мы говорил ранее, позволяют выделять локальные признаки из последовательности.
Благодаря объединению этих двух механизмов, мы получаем производительную архитектуру.
Главная проблема заключалась в том, что аудио имеют большой размер последовательности.
А трансформеры, в силу своей архитектуры, масштабируются квадратично, так как должны проверять зависимость токенов каждый от каждого.
Взяв идеи из Longformer, а именно скользящее окно внимания и глобальный токен, в Nvidia ограничили <<видимость>> токенов в механизме внимания и уменьшили затраты вычислительных ресурсов.
Теперь каждый токен смотрит только на себя и на ближайших соседей.
Также модель обучалась с использованием Transducer архитектуры.

\section{Методы улучшения качества распознания речи}
Первым на ум приходит добавить больше данных для обучения.
Так мы разнообразим выборку новыми словами и в теории улучшим точность этого самого распознания.
Однако тут возникает сразу несколько проблем. Самое главное - сложность нахождения размеченного датасета пар аудио-транскрипция.
При этом размеченные данные должны совпадать с использовавшимися до этого.
То есть наличие знаков препинания, совпадение регистра и правила написания чисел (цифрами или буквами).
Продолжим тем, что аудио сами по себе занимают много места в дисковом пространстве.
Если мы берём стандартное аудио, то есть частота дескритизации равна 16000 Гц.
Это значит, что одна секунда аудио кодируется 16000 числами.

Методы пост-коррекции начали появляться ещё до большого развития нейронных сетей.
Один из методов, который появился ещё 1990-е был ROVER.
Данная методика является своеобразным alighnment.
При спользование этого метода, мы берём n-гипотез. 
Затем мы формируем начинаем формировать единую гипотезу.
Мы проходим по токенам наших n-гипотез и берём самый часто встречающийся.
Проблема данной методики заключается в том, что мы предполагаем, что наша ASR модель не имеет проблем с распознанием отдельным слов.
Если у нас в обучающих данных отсутствовало какое-то слово, то модель может распознавать его хуже других.

Методом, который используется и в наши дни является BeamSearch.
При жадом (greedy) декодирование мы каждый раз выбираем следующий самый вероятный токен в качестве следующего элемента генерируемой последовательности.
Однако это не всегда допустимо.
Может оказаться, что другая последовательность декодирования может иметь большую вероятность.
При декодирование с помощью BeamSearch мы одновременно строем n последовательностей, где n - параметр $beam size$.
Таким образом на выходе мы получаем n последовательностей и их вероятностей по формуле~\ref{eq:probs}:
\begin{equation}
  P(y|x) = P(y|x_1,x_2,\dots x_i)
  \label{eq:probs}
\end{equation}

И берём самую вероятную последовательность.
Проблема метода заключается в том, что он полагается на внутренние знания языка у модели транскрибации аудио.

Другим методом улучшения качества распознания, уже использующих языковые модели, является рескоринг (shallow fusion).
При этой методике, подобно Transducer, обучается внешняя языковая модель.
Берётся большой текстовый корпус, который кодируется токенизатором ASR модели.
Затем обучается языковая модель на этом тексте.
Таким образом мы получаем LM, которая имеет внутреннее контекстное представление о вероятности следующего токена на основание предыдуших.
По своей сути тут решается уже задача уменьшения перплексии, то есть текстовой неопределённости, формула которой выглядит так~\ref{eq:perp}:

\begin{equation}
  P(x) = P(x_1)P(x_2)\dots P(x_i) = \prod_{i=1}^{n}P(x_i)
  \label{eq:perp}
\end{equation}

Преимуществом рескоринга является то, что он совместим и с другими методами улучшения качества распознания речи, такими как ROVER или BeamSearch.
Также найти качественный корпус текста в интернете сильно проще, чем размеченные аудио данные.
Вместе с тем его главным недостатком является завязанность на одной модели.
Мы не можем обучить языковую модель для рескоринга для разных ASR, поскольку LM строго завязана на токенизаторе.
То есть, если мы решим взять ту же модель, но при этом поменяем местами два токена, наша языковая модель будет бесполезна.

\section{Цели и задачи бакалаврской работы}

\textbf{Цель работы} -- дообучить языковую модель для коррекции транскрипций аудио, полученных из систем траскрибации аудио.

\textbf{Задачи работы:}
\begin{enumerate}
  \item Дообучить языковую модель.
  \item Проверить переносимость знаний между разными ASR.
  \item Сравнить результаты с узкоспециализированными методами.
\end{enumerate}
