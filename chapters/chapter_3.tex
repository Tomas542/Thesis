\newpage
\begin{center}
  \textbf{\large 3. РАЗРАБОТКА СОБСТВЕННОГО МЕТОДА}
\end{center}
\refstepcounter{chapter}
\addcontentsline{toc}{chapter}{3. РАЗРАБОТКА СОБСТВЕННОГО МЕТОДА}

Наша основная идея в обучение языковой модели на русском языке для исправление ошибок.
Как мы обсуждали ранее, в основном методы, завязанные на LM полагаются на скрытое представление ASR модели или же на её проекцию в скрытое представление языковой модели.
Мы же будем проводить наши тесты с использованием естественного языка.
То есть мы будем сначала транскрибировать аудио в текст и только потом его править.
Таким образом мы решим проблему универсальности подхода и независимости ASR-LM пары.

В качестве двух моделей мы возьмём ruT5 и Fred-T5.
Обе модели были разработаны Сбером и обучены на корпусе русского языка.
Они построены на архитектуре энкодер-декодер трансформеров.
Их мы выбрали по нескольким причинам:

\begin{enumerate}
  \item \textbf{Размер}.
  Мы берём Base (220М) и distil (95М) версии соответственно. 
  В паре с fastconformer это займёт немногим больше 3 Гб видеопамяти, что позволит влезть в любую пользовательскую видеокарту.
  \item \textbf{Архитектура}.
  Энкодер-декодер лучше улавливает ошибки и корректирует их.
  Такие выводы были сделаны на основе решения этой задачи для английского языка, где в качестве моделей-корректоров использовались Mistral-7b и Llama3-8b.
\end{enumerate}

Теперь необходимо создать датасет для обучения.
Для этого возьмём и сгенерируем 5 гипотез с помощью BeamSearch метода.
Затем мы подадим все 5 гипотез в нашу модель и попросим сгенерировать исправленную последовательность.
Обучение ruT5 будет проходить в few-shot сценарии, когда мы ей будем показывать 3 примера из валидационной выборке в качестве примера для улучшения.
Так мы проведём 15 обучающих эпох и сохраним модель с наименьшим WER на валидационной выборке.

Почему подавать 5 гипотез в модель? Что мы можем взять из гипотез?
Почему просто не реранжировать гипотезы?
Давайте проанализируем полученные данные.

\textbf{Тут графки.}

Сначала разъясним, почему не реранжирование.
Первый график на Рис. 1 показывает процент случаев, когда гипотеза 2-5 имеют ниже WER, чем 1 гипотеза.
Их процент колеблется от 4 до 8, что значит мы можем не получить значительного прироста.
Второй график же показывает процент токенов, которые:
\begin{itemize}
  \item Есть в праильной транскрипции.
  \item Есть в гипотезе $n_k$ и нет в гипотезах $n_i$, где $i < k$.
\end{itemize}

Это та самая информация, которую наша модель может извлечь из сгенерированных гипотез и улучшить качество транскрипции.
Именно поэтом мы переходим к обучению отдельной языковой модели на ошибках.

Обучение будем проводить с использованием метода Low-rank adaptaion (LoRA).
Это метод, при котором изначальные веса модели в выбранных слоях замораживаются и никак не изменяются в ходе дообучения.
В нашем случае такими слоями будут слои attention.
У каждой матрицы attention (Q, K, V, O) инициализируется своя пара матриц A и B размерами (h, rank) и (rank, h), где h – размер скрытого пространства матрицы attention, а rank – размер ранга, заданный при LoRA.
Во время обучения будут изменяться как раз матрицы A и B, которые как бы представляют собой сжатую информацию из весов слоя.
Преимуществом LoRA является то, что мы значительно экономим в используемой памяти, жертвуя качеством.
Это происходит благодаря тому, что в state оптимизатора хранятся не матрицы размера h $\times$ h, а две матрицы размеров h $\times$ rank и rank $\times$ h.
Баланс между памятью и качеством достигается за счёт настройки параметра ранга для LoRA.
Чем больше это значение, тем более точное представление изначального слоя мы получаем, но вместе с тем больше памяти будет храниться в state оптимизатора.
С формулой можно ознакомиться в уравнение~\ref{eq:lora}
\begin{equation}
  W_{new} = W_{frozen} + A \times B
  \label{eq:lora}
\end{equation}

Размер batch'а был задан равным 64, использовался линейный планировщик с разогревом.
Также стоит сказать, что модель обучалась исключительно на ошибках Fastconformer модели.
Соответственно результаты для модели Whisper в связке с ruT5 будут скорее результатов переноса способностей к коррекции между разными моделями в рамках одного набора данных.

Помимо обученного T5 мы применяли Fred-T5 дистилированный, который используется как инструмент для восстановления знаком препинания.
Это сделано для проверки способностей ASR моделей генерировать последовательности правильные с точки зрения пунктуации.

\begin{table}[]
\centering
\caption{Сравнение значений WER. ruT5 модель была натренирована на CV21 и RuLS.}
\begin{tabular}{|c|c|ccc|}
\hline
\multirow{2}{*}{Модель}        & \multirow{2}{*}{Сеттинг}             & \multicolumn{3}{c|}{Датасет, WER}                                    \\ \cline{3-5} 
                               &                                      & \multicolumn{1}{c|}{CV21}  & \multicolumn{1}{c|}{RuLS}     & OpenSTT \\ \hline
\multirow{6}{*}{Fastconformer} & Greedy                               & \multicolumn{1}{c|}{12.31} & \multicolumn{1}{c|}{38.43}    & -       \\ \cline{2-5} 
                               & BeamSearch                           & \multicolumn{1}{c|}{12.26} & \multicolumn{1}{c|}{38.84}    & -       \\ \cline{2-5} 
                               & LM                                   & \multicolumn{1}{c|}{9.87}  & \multicolumn{1}{c|}{-}    & -       \\ \cline{2-5} 
                               & Greedy+Distill                       & \multicolumn{1}{c|}{13.81} & \multicolumn{1}{c|}{42.16}    & -       \\ \cline{2-5} 
                               & BeamSearch+Distill                   & \multicolumn{1}{c|}{3.82}  & \multicolumn{1}{c|}{-}    & -       \\ \cline{2-5} 
                               & BeamSearch+ruT5                      & \multicolumn{1}{c|}{-}     & \multicolumn{1}{c|}{-}    & -       \\ \hline
\multirow{4}{*}{Whisper}       & Greedy                               & \multicolumn{1}{c|}{19.60} & \multicolumn{1}{c|}{-}    & -       \\ \cline{2-5} 
                               & BeamSearch                           & \multicolumn{1}{c|}{18.33} & \multicolumn{1}{c|}{35.49}    & -       \\ \cline{2-5} 
                               & Greedy+Distill                       & \multicolumn{1}{c|}{20.57} & \multicolumn{1}{c|}{-}    & -       \\ \cline{2-5} 
                               & BeamSearch+ruT5                      & \multicolumn{1}{c|}{-}     & \multicolumn{1}{c|}{-}    & -       \\ \hline
\end{tabular}
\label{tab:res_full}
\end{table}

Проанализируем результаты Таблицы~\ref{tab:res_full}.


\begin{table}[]
\centering
\caption{Сравнение значений WER. ruT5 модель была натренирована исключительно на CV21.}
\begin{tabular}{|c|c|ccc|}
\hline
\multirow{2}{*}{Модель}        & \multirow{2}{*}{Сеттинг}             & \multicolumn{3}{c|}{Датасет, WER}                                    \\ \cline{3-5} 
                               &                                      & \multicolumn{1}{c|}{CV21}  & \multicolumn{1}{c|}{RuLS}     & OpenSTT \\ \hline
\multirow{2}{*}{Fastconformer} & BeamSearch                           & \multicolumn{1}{c|}{12.26} & \multicolumn{1}{c|}{38.34}    & -       \\ \cline{2-5} 
                               & BeamSearch+ruT5                      & \multicolumn{1}{c|}{3.20}  & \multicolumn{1}{c|}{40.07}    & -       \\ \hline
\multirow{2}{*}{Whisper}       & BeamSearch                           & \multicolumn{1}{c|}{18.33} & \multicolumn{1}{c|}{35.49}    & -       \\ \cline{2-5} 
                               & BeamSearch+ruT5                      & \multicolumn{1}{c|}{4.99}  & \multicolumn{1}{c|}{38.04}    & -       \\ \hline
\end{tabular}
\label{tab:res_cv_trained}
\end{table}

Взглянув на Таблицу~\ref{tab:res_cv_trained}, мы можем сделать несколько выводов.
Во-первых — подход с коррекцией текстовой расшифровки аудио показывает себя лучше всех.
Это значит, что богатые лингвистические знания модели + дообучение под нашу задачу работают отлично.
Во-вторых можно увидеть, что перенос знаний между разными ASR моделями также работает.
Так наша модель смогла в 4 раза улучшить качество распознавания для Whisper, хотя до этого она обучалась исключительно на результатах Fastconformer.
Также в обоих случаях использование дистиллированной модели при сценарии жадного декодирования не принесло никакого прироста в качестве, а наоборот показало ухудшение. Причин такого падения назвать наверняка нельзя.
Можно сказать, что ASR модели, используя свои знания о языке, имеют более точное представление о правописание и пунктуации.
Или же distill модель пыталась поменять какое-то слово, которое в изменение не нуждалось, из-за чего WER вырос.

По итогу можно уверенно сказать, что наш метод показал в разы лучшее качество в сравнение с другими методами и при этом затраты вычислительных ресурсов, требуемых для inference пары Fastconformer + ruT5 сравнительно не велико.
В сумме обе эти модели помещаются в 3-4 Гб видеопамяти, что в современном мире является минимумом для самых низкобюджетных карт, в том числе и среди ноутбучных.
